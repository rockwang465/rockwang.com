---
# Variables here are applicable to all host groups
# hostname: k8s-test-67


#################################### init environment #################################
ntpserver: 'ntp.api.bz'
IP: "{{ ansible_eth0['ipv4']['address'] }}"
# IP: {{ ansible_default_ipv4.address }}

bin_dir: /usr/bin
files_dir: ~/k8s  #拷贝临时存放文件路径，如dashboard.yaml k8s.sh等文件.


#--------------- offline repos ---------------
repos:
  local: disable  #enable/disable
  package_path: /tmp/sensenebula-guard-1.0
  yum:
    domain: infra.yum.com
    repo_10: 10.5.6.10:6000
    port: 7000
    data: /data/yum_data

  docker:
    data: /data/docker-registry/
    ip: "{{ IP }}"  #这里得确认是哪台机器作为离线仓库，是本地，还是其他的局域网机器. kubernetes/defaults/main.yaml引用了.
    port: 5000

  helm:
    name: chartmuseum
    data: /data/charts
    port: 8080
    package: helm_charts/charts_package

chartmuseum: "http://{{ IP }}:8080"  # 暂时没有离线仓库，所以先用下面的临时用下.

py3:
  off_line: enable
#---------------------------------------------


docker_registry: 10.5.6.10

#kernel_update_version: 3.10.0-693.5.2.el7.x86_64


#################################### bootstrap       ####################################
disk_parted_rules:
  nebula_disks:
    - name: /dev/sdb
      wipe: false
      parts:
       - bdev: /dev/sdb1
         capacity:
           start: 1MB
           end: 10000MB           # 10G
         path: /mnt/locals/mysql/volume0
       - bdev: /dev/sdb2
         capacity:
           start: 10000MB
           end: 20000MB           # 10G
         path: /mnt/locals/redis/volume0
       - bdev: /dev/sdb3
         capacity:
           start: 20000MB
           end: 30000MB           # 10G
         path: /mnt/locals/prometheus/volume0

    #   - bdev: /dev/sdb1
    #     capacity:
    #       start: 1MB
    #       end: 10000MB           # 10G
    #     path: /mnt/locals/engine/volume0
    #   - bdev: /dev/sdb2
    #     capacity:
    #       start: 10000MB
    #       end: 20000MB           # 10G
    #     path: /mnt/locals/engine/volume1
    #   - bdev: /dev/sdb3
    #     capacity:
    #       start: 20000MB
    #       end: 110000MB           # 90G
    #     path: /mnt/locals/cassandras/volume0
    #   - bdev: /dev/sdb4
    #     capacity:
    #       start: 110000MB
    #       end: 200000MB           # 90G
    #     path: /mnt/locals/cassandras/volume1
    #   - bdev: /dev/sdb5
    #     capacity:
    #       start: 200000MB
    #       end: 230000MB           # 30G
    #     path: /mnt/locals/kafkas/volume0
    #   - bdev: /dev/sdb6
    #     capacity:
    #       start: 230000MB
    #       end: 238000MB           # 8G
    #     path: /mnt/locals/seeweedfsmaster/volume0
    #- name: /dev/sdc
    #  wipe: false
    #  parts:
    #   - bdev: /dev/sdc1
    #     capacity:
    #       start: 1MB
    #       end: 50000MB           # 50G
    #     path: /mnt/locals/zookeepers/volume0
    #   - bdev: /dev/sdc2
    #     capacity:
    #       start: 50000MB
    #       end: 550000MB           # 500G
    #     path: /mnt/locals/mysql/volume0
    #   - bdev: /dev/sdc3
    #     capacity:
    #       start: 550000MB
    #       end: 600000MB           # 50G
    #     path: /mnt/locals/alertmanager/volume0
    #   - bdev: /dev/sdc4
    #     capacity:
    #       start: 600000MB
    #       end: 800000MB           # 200G
    #     path: /mnt/locals/prometheus/volume0
    #   - bdev: /dev/sdc5
    #     capacity:
    #       start: 800000MB
    #       end: 5800000MB           # 5T
    #     path: /mnt/locals/elasticsearch/volume0
    #   - bdev: /dev/sdc6
    #     capacity:
    #       start: 5800000MB
    #       end: 12000000MB           # 6T
    #     path: /mnt/locals/seeweedfsvolume/volume0

disk_parted_rule: nebula_disks


#################################### kubernetes vars ####################################
# docker_ce_version: 'docker-ce-18.06.1.ce-3.el7'
docker_ce_version: 'docker-ce-18.09.4-3.el7'
docker_cgroup_driver: 'systemd'
docker_insecure_registry:
  - 192.168.0.0/24
  - 10.5.1.0/24
# docker_default_runtime: 'nvidia'
# docker_daemon_graph: '/var/lib/docker'

kube_tools_version: '1.14.0-0'  #kubelet kubectl kubeadm

k8s_version:
  # v1.14.0 versions可以去此地址看: https://www.kubernetes.org.cn/5213.html
  kube: 'v1.14.0'  #images version and kubeadm init version
  coredns: '1.3.1' # coredns: '1.2.6'
  etcd: '3.3.10'  # etcd: '3.2.24'
  pause: '3.1'
  
  flannel_amd64: 'v0.10.0-amd64'
  kubernetes_dashboard_amd64: 'v1.10.1'
  # tiller: v2.13.1  #这个是k8s v1.14.0版本的tiller.  由于helm客户端和tiller服务端版本不匹配，我没时间去改，所以用2.11.0版本。 1.14.0版本地址记录: gcr.io/kubernetes-helm/tiller:v2.13.1  # registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.13.1
  tiller: 'v2.11.0'
  local_volume: v2.1.0  # local_volume: v2.3.0  这里的镜像后面已经导入了，所以这里不设置也没事.

  traefik: '1.7.9'
  metircs_server_adm64: 'v0.3.1'
  addon_resizer: '1.8.4'

kube_proxy_mode: ipvs

kube_config_dir: /etc/kubernetes

iface: eth0

dashboard_nodeport: 30000

addons_charts:
  - name: local-volume-provisioner
  #- name: provisioner
    version: 2.3.0-master-1e4ec68
    namespace: kube-system
  # - name: kubernetes-dashboard
  #   version: 1.10.1-master-7cd5efa
  #   namespace: kube-system
  # - name: nginx-ingress
  #   version: 0.21.0-master-8128f23
  #   namespace: kube-system

devops_charts:
  - name: elasticsearch
    version: 6.5.4-master-53b56e8
    namespace: logging
  - name: prometheus-operator
    version: 2.0.0-master-95af25b
    namespace: monitoring
  - name: jaeger-operator
    version: 1.10.0-master-d096d64
    namespace: monitoring




deploy_mode: standalone   # standalone or distributed

